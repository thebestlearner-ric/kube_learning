# Cluster Admin
1. Node Shutdowns
2. Certificates
3. Logging Architecture
4. Metrics for Kubernetes System Components
5. Metrics for Kubernetes Object States
6. System Logs
7. Traces for Kubernetes system
8. Proxies in Kubernetes
9. API Priority and Fairness
10. Cluster Autoscaling
11. Installing Addons

## Node Shutdowns
Reasons for shutdown: 
- Power outage at the Datacenter
- Planned maintainance 
2 types of Shutdowns
1. Graceful
2. Non-graceful

## What happens during a shutdown
1. Systemd detects or notifies node shutdown
2. kubelet sets a `NotReady` with `reason: "node is shutting down"`
3. kube-scheduler honors this condition and does not schedule any Pod to the Node
4. kubelet also reject Pods during the `PodAdmission` phase
5. kubelet will begin terminating Pods that are running locally
- Even Pods with toleration: `node.kubernetes.io/not-ready:NoSchedule` will not be scheduled

### Graceful shutdown
By default: Not active. To activate set `KubeletConfiguration` below to non-zero values
The gracefull shutdown depends on the systed inhibitor locks to delay the node shutdown with a given duration
`KubeletConfiguration`:
- `shutdownGracePeriod` default value:0 (available from v1.21)
- `shutdownGracePeriodCriticalPods` default value:0
During a graceful shutdown, kubelet will terminate pods in two phases:
1. Terminate regular pods running on the node.
2. Terminate critical pod running
Examples:
- `shutdownGracePeriod=30s` 
- `shutdownGracePeriodCriticalPods=10s`
Kubelet will delay the node shutdown by 30secs, during the shutdown the first 20 seconds reserved for gracefully terminating normal pods, and the last 10 seconds would be reserved for terminating critical pods.

### Non-Graceful shutdown
Too complex. input later

## Certificates
Certificates that allow internal and external cluster communication

## Cluster Networking
Kubernetes is all about sharing machines among applications. Typically, sharing machines requires ensuring that two applications do not try to use the same ports. Coordinating ports across multiple developers is very difficult to do at scale and exposes users to cluster-level issues outside of their control.

Dynamic port allocation brings a lot of complications to the system - every application has to take ports as flags, the API servers have to know how to insert dynamic port numbers into configuration blocks, services have to know how to find each other, etc. Rather than deal with this, Kubernetes takes a different approach.
### Distinct Networking Problems
4 Distinct Networking Problems:
1. Highly-coupled container to container communications: this solved by `Pods` and `localhost` communications
2. Pod to pod communications
3. Pod to service communications
4. External to service communications

## Logging Architecture
Cluster-level logging architectures require a separate backend to store, analyze, and query logs. Kubernetes does not provide a native storage solution for log data. Instead, there are many logging solutions that integrate with Kubernetes. The following sections describe how to handle and store logs on nodes.

Note: Pod writes logs to the `stdout` and `stderr`.
You can use `kubectl logs --previous` to retrieve logs from a previous instantiation of a container. If your pod has multiple containers, specify which container's logs you want to access by appending a container name to the command, with a `-c` flag, like so:
```sh
# count is the name of the container within the pod
kubectl logs counter -c count
```

### Log rotation
You can configure two kubelet configuration settings, `containerLogMaxSize` (default 10Mi) and `containerLogMaxFiles` (default 5), using the kubelet configuration file. These settings let you configure the maximum size for each log file and the maximum number of files allowed for each container respectively.

In order to perform an efficient log rotation in clusters where the volume of the logs generated by the workload is large, kubelet also provides a mechanism to tune how the logs are rotated in terms of how many concurrent log rotations can be performed and the interval at which the logs are monitored and rotated as required. You can configure two kubelet configuration settings, `containerLogMaxWorkers` and `containerLogMonitorInterval` using the kubelet configuration file.

#### Examples of Log rotation
For example, if a Pod writes 40 MiB of logs and the kubelet rotates logs after 10 MiB, running `kubectl logs` returns at most 10MiB of data.

#### System Component logs
Two types of system components: 
1. Container 
    - The kubelet and container runtime do not run in containers. The kubelet runs your containers (grouped together in pods)
2. Things that run the container
    - The Kubernetes scheduler, controller manager, and API server run within pods (usually static Pods). The etcd component runs in the control plane, and most commonly also as a static pod. If your cluster uses kube-proxy, you typically run this as a `DaemonSet`.

## Example Application of a Logging architecture
Using:
1. Prometheus - Collection and monitoring metrics
2. Thanos - For providing long-term storage and high availablity for Prometheus metrics
3. Loki - for aggregating and quering logs
4. Grafana - for visualising both metrics and logs

Prometheus, Thanos, and Loki are all tools related to monitoring, logging, and observability, but they serve different purposes and are used in different contexts. Here's a breakdown of each:

### Prometheus

- **Purpose**: Time-series database and monitoring system.
- **Functionality**:
  - Collects and stores metrics data from applications and infrastructure.
  - Uses a pull-based model to scrape metrics from endpoints.
  - Supports powerful querying using PromQL (Prometheus Query Language).
  - Can generate alerts based on metrics data and integrates with Alertmanager.
- **Use Case**: Real-time monitoring and alerting for infrastructure and applications.

### Thanos

- **Purpose**: Scalable, long-term storage for Prometheus metrics.
- **Functionality**:
  - Extends Prometheus by adding long-term storage capabilities and high availability.
  - Allows multiple Prometheus instances to be queried as a single data source.
  - Provides downsampling for historical data to reduce storage requirements.
  - Supports global querying across multiple Prometheus instances.
- **Use Case**: When you need to retain Prometheus metrics data for the long term and require scalable, highly available metrics storage and querying.

### Loki

- **Purpose**: Log aggregation system, often described as "Prometheus for logs."
- **Functionality**:
  - Collects and stores log data from applications and infrastructure.
  - Uses a pull-based model to collect logs, similar to how Prometheus scrapes metrics.
  - Supports powerful querying using LogQL (Loki Query Language).
  - Integrates with Grafana for log visualization and correlation with metrics.
- **Use Case**: Centralized logging solution to aggregate, store, and query log data. Particularly useful for correlating logs with metrics.

### Comparison

| Feature        | Prometheus                       | Thanos                                | Loki                              |
|----------------|----------------------------------|---------------------------------------|-----------------------------------|
| **Purpose**    | Time-series metrics monitoring   | Long-term storage for Prometheus      | Log aggregation and querying      |
| **Data Type**  | Metrics                          | Metrics (Prometheus)                  | Logs                              |
| **Query Language** | PromQL                        | PromQL                                | LogQL                             |
| **Storage**    | Local storage                    | Object storage (e.g., S3, GCS)        | Local or object storage           |
| **Scalability**| Limited by single node capacity  | Scalable, multi-cluster, high availability | Scalable, depends on backend      |
| **High Availability** | Not inherently highly available | Built-in support for high availability | Supports high availability       |
| **Integration**| Alertmanager, Grafana            | Grafana, Prometheus                   | Grafana                           |
| **Retention**  | Short to medium term             | Long term                             | Configurable                      |
| **Use Case**   | Real-time monitoring and alerting | Long-term metrics storage and HA      | Centralized logging               |

### When to Use Each

- **Prometheus**: Use for real-time monitoring and alerting of metrics from your applications and infrastructure.
- **Thanos**: Use in conjunction with Prometheus when you need long-term storage, scalability, and high availability for your metrics data.
- **Loki**: Use for centralized log aggregation and querying, especially when you want to correlate logs with metrics data from Prometheus.

### Example Setup

In a comprehensive observability setup, you might use all three tools together:

1. **Prometheus** for collecting and monitoring metrics.
2. **Thanos** for providing long-term storage and high availability for Prometheus metrics.
3. **Loki** for aggregating and querying logs, with Grafana used to visualize both metrics and logs.

This combination provides a powerful and flexible observability stack that can handle metrics, logs, and long-term storage, all integrated through Grafana for unified visualization and analysis.